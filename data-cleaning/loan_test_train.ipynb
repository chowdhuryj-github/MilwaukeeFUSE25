{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fecd715e",
   "metadata": {},
   "source": [
    "# <span style=\"color: #0F749B\"> Data Cleaning for Loan Test and Train Datasets </span>\n",
    "This Jupyter Notebook covers the data cleaning process for the loan train and loan test datasets. Here, we look for features that have missing values, determine methods of imputation and use other data cleaning techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77276f27",
   "metadata": {},
   "source": [
    "## <span style=\"color: #04A4E2\"> The Necessary Imports </span>\n",
    "Before staring anything, we need to first import the necessary python libraries so that we can perform the data cleaning process. The following libraries we need to import are `numpy`, `pandas` and `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5bc2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all required imports\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ecbf3d",
   "metadata": {},
   "source": [
    "## <span style=\"color: #F4AB04\"> Loading the Dataset </span>\n",
    "Next, we move on to loading the dataset, as well as looking at the description and information behind the dataset so that we know if null values there are or if any data cleaning is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d69ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data frames and displaying them\n",
    "train_df = pd.read_csv(\"../data/loan-train.csv\")\n",
    "test_df = pd.read_csv(\"../data/loan-test.csv\")\n",
    "\n",
    "display(train_df)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f5b2f",
   "metadata": {},
   "source": [
    "### <span style=\"color: #F4AB04\"> A Deeper Dive into the Training Dataset </span>\n",
    "To gain a better understanding of the dataset we're dealing with, we need to look deeper into the statistics of the dataset itself. The next cells give us a better idea of how the training dataset looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the information and descriptive statistics\n",
    "display(train_df.info())\n",
    "display(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b700e60",
   "metadata": {},
   "source": [
    "### <span style=\"color: #F4AB04\"> A Deeper Dive into the Testing Dataset </span>\n",
    "To gain a better understanding of the dataset we're dealing with, we need to look deeper into the statistics of the dataset itself. The next cells give us a better idea of how the testing dataset looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5832b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the information and descriptive statistics\n",
    "display(test_df.info())\n",
    "display(test_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd2e6ae",
   "metadata": {},
   "source": [
    "### <span style=\"color: #F4AB04\"> Observations and Conclusion </span>\n",
    "When looking the statistics behind the dataset, we can see that both the train and test datasets are very similar. As a result, it is time saving to inspect one dataset, make data cleaning decisions and apply to both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069682d6",
   "metadata": {},
   "source": [
    "## <span style=\"color: #04A4E2\"> Missing Value Imputations </span>\n",
    "In these next steps, we look for missing values within the dataset and then decide on a method of imputation. This will allow us to fill out the missing values and ensure a more complete dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7fbcc2",
   "metadata": {},
   "source": [
    "### <span style=\"color: #04A4E2\">Features with Null Values & Skew </span>\n",
    "First, we want to look for features with null values. Once we find those features with null values, we investigate the skew of the dataset by either visualizing them or using `.skew()` and then deciding on a method of imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dbee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping Loan_ID for train \n",
    "loan_ids = train_df['Loan_ID'].copy()\n",
    "train_df.drop(columns=['Loan_ID'], inplace=True)\n",
    "\n",
    "# intializing a list\n",
    "null_train_numerical_list = []\n",
    "null_train_categorical_list = []\n",
    "\n",
    "# select the data tyes\n",
    "train_numeric_features = train_df.select_dtypes(include='number')\n",
    "train_categorical_features = train_df.select_dtypes(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e42c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping Loan_ID for now\n",
    "loan_ids = test_df['Loan_ID'].copy()\n",
    "test_df.drop(columns=['Loan_ID'], inplace=True)\n",
    "\n",
    "# intializing a list\n",
    "null_test_numerical_list = []\n",
    "null_test_categorical_list = []\n",
    "\n",
    "# select the data tyes\n",
    "test_numeric_features = test_df.select_dtypes(include='number')\n",
    "test_categorical_features = test_df.select_dtypes(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dac22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a for loop that loops through numeric train features\n",
    "for column in train_numeric_features:\n",
    "\n",
    "    if train_df[column].isnull().any():\n",
    "\n",
    "        # add to the null numerical features list\n",
    "        null_train_numerical_list.append(column)\n",
    "        null_test_numerical_list.append(column)\n",
    "\n",
    "        # applies imputations for both datasets\n",
    "        if(train_df[column].skew() > 0):\n",
    "            print(f\"The feature {column} is right skewed\")\n",
    "            train_df[column] = train_df[column].fillna(train_df[column].median())\n",
    "            test_df[column] = test_df[column].fillna(test_df[column].median())\n",
    "\n",
    "        if(train_df[column].skew() < 0):\n",
    "            print(f\"The feature {column} is left skewed\")\n",
    "            train_df[column] = train_df[column].fillna(train_df[column].median())\n",
    "            test_df[column] = test_df[column].fillna(test_df[column].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee98c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a for loop that loops through the categorical train features\n",
    "for column in train_categorical_features:\n",
    "    \n",
    "    if train_df[column].isnull().any():\n",
    "\n",
    "        print(f\"The feature {column} has null values!\")\n",
    "\n",
    "        # add to the null categorical features list\n",
    "        null_train_categorical_list.append(column)\n",
    "        null_test_categorical_list.append(column)\n",
    "\n",
    "        # applies imputations for both datasets\n",
    "        train_df[column] = train_df[column].fillna(train_df[column].mode()[0])\n",
    "        test_df[column] = test_df[column].fillna(test_df[column].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e78ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.isnull().values.any())\n",
    "print(test_df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e78e13",
   "metadata": {},
   "source": [
    "## <span style=\"color: #F4AB04\"> Converting Categorical Features to Numerical </span>\n",
    "Next, we would like to perform one hot encoding over categorical features. If the categorical feature has only two unique values, we can map 1 and 0, else if it has more than two unique values, then we use one hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a03bb",
   "metadata": {},
   "source": [
    "### <span style=\"color: #F4AB04\"> Categorical Features with Two Unique Values </span>\n",
    "To begin, we look for categorical features with two unique values, and then apply the mapping of 1's and 0's to those features itself. No one hot encoding required here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54de6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for looping through all categorical features\n",
    "for feature in train_categorical_features:\n",
    "    \n",
    "    if feature not in test_df.columns:\n",
    "        continue\n",
    "    \n",
    "    if train_df[feature].nunique() == 2:\n",
    "        print(f\"The feature {feature} has two unique values!\")\n",
    "        unique_train_values = train_df[feature].unique()\n",
    "        unique_test_values = test_df[feature].unique()\n",
    "\n",
    "        unique_train_mapping = {unique_train_values[0]: 0, unique_train_values[1]: 1}\n",
    "        unique_test_mapping = {unique_test_values[0]: 0, unique_test_values[1]: 1}\n",
    "\n",
    "        # apply mapping and change data type for train\n",
    "        train_df[feature] = train_df[feature].map(unique_train_mapping)\n",
    "        train_df[feature] = train_df[feature].astype(\"int\")\n",
    "        \n",
    "        # apply mapping and change data type for test\n",
    "        test_df[feature] = test_df[feature].map(unique_test_mapping)\n",
    "        test_df[feature] = test_df[feature].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53a3cd",
   "metadata": {},
   "source": [
    "### <span style=\"color: #F4AB04\"> Categorical Features with More Than Two Unique Values</span>\n",
    "To begin, we look for categorical features with more than two unique values, and as a result, we perform the one hot encoding process, so that we can make the dataset fit for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af32e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping through the categorical features\n",
    "for feature in train_categorical_features:\n",
    "    \n",
    "    if train_df[feature].nunique() > 2:\n",
    "\n",
    "        print(f\"The feature {feature} has more than two unique values!\")\n",
    "\n",
    "        train_encode = pd.get_dummies(train_df[feature], prefix=feature, dtype=int)\n",
    "        train_df.drop(columns=[feature], inplace=True)\n",
    "        train_df = pd.concat([train_df, train_encode], axis=1)\n",
    "\n",
    "# adding back loan ID's to the dataset\n",
    "train_df.insert(0, 'Loan_ID', loan_ids)\n",
    "\n",
    "# displaying the new dataframe\n",
    "train_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
